Giving myself evidence of obvious concepts in Neural Networks
    I want to answear some pretty obvious questions, that have pretty obvious answear, but I want to see it for myself. I always want to take it as an opportunity to look into the vizulazation of neural networks, which is a field that has most interest to me then the actual uses of neural networks. 

    Question 1. 
        Does training a network for more epochs have a linear gain, or does it tail off?

    Question 2.
        Does the structure of neural networks have a noticiable effect of the accuracy of a network?

    Question 3.
        How does the data feed to a network change the internal structure?

    Questions 4.
        Do neural networks converge to similar shapes if trained on the same iput with different starting points
    
Types of Networks
    Feedforward
    AutoEncoder
    Probablistic
    Time Delay
    Convolutional
    Regulatory Feedback
    Radial Basis Function